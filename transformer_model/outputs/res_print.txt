============================================================
High Risk Customer Prediction - Transformer Model
Using Self-Attention Mechanism for Tabular Data
============================================================
Loading data: train.csv
Data dimensions: (80000, 51)
Loading data: test.csv
Data dimensions: (20000, 50)

Starting training data preprocessing...
Original data dimensions: (80000, 51)
After removing samples with >50% missing variables: (80000, 51)
Final feature count: 40
Feature dimensions: (80000, 40), Label dimensions: (80000,)
Positive sample ratio: 0.3683
Negative sample ratio: 0.6318

Dataset Split:
Training set: (64000, 40), Positive ratio: 0.3683
Validation set: (16000, 40), Positive ratio: 0.3683

Transformer Model Structure:
TabularTransformer(
  (feature_embedding): Linear(in_features=1, out_features=128, bias=True)
  (pos_encoding): PositionalEncoding()
  (encoder_layers): ModuleList(
    (0-3): 4 x TransformerEncoderLayer(
      (attention): MultiHeadAttention(
        (W_q): Linear(in_features=128, out_features=128, bias=True)
        (W_k): Linear(in_features=128, out_features=128, bias=True)
        (W_v): Linear(in_features=128, out_features=128, bias=True)
        (W_o): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): FeedForward(
        (linear1): Linear(in_features=128, out_features=512, bias=True)
        (linear2): Linear(in_features=512, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Sequential(
    (0): Linear(in_features=128, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=256, out_features=128, bias=True)
    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (6): ReLU()
    (7): Dropout(p=0.1, inplace=False)
    (8): Linear(in_features=128, out_features=1, bias=True)
    (9): Sigmoid()
  )
)
Parameter count: 860,161

Starting Transformer model training...
Training set: 64000, Validation set: 16000
Epoch [5/100] - Train Loss: 0.0272, Val Loss: 0.0267, Val AUC: 0.8843, Val F1: 0.7147
Epoch [10/100] - Train Loss: 0.0264, Val Loss: 0.0259, Val AUC: 0.8914, Val F1: 0.7512
Epoch [15/100] - Train Loss: 0.0260, Val Loss: 0.0257, Val AUC: 0.8925, Val F1: 0.7502
Epoch [20/100] - Train Loss: 0.0259, Val Loss: 0.0255, Val AUC: 0.8930, Val F1: 0.7474
Epoch [25/100] - Train Loss: 0.0256, Val Loss: 0.0252, Val AUC: 0.8949, Val F1: 0.7417
Epoch [30/100] - Train Loss: 0.0254, Val Loss: 0.0251, Val AUC: 0.8955, Val F1: 0.7463
Epoch [35/100] - Train Loss: 0.0253, Val Loss: 0.0252, Val AUC: 0.8942, Val F1: 0.7449
Epoch [40/100] - Train Loss: 0.0252, Val Loss: 0.0253, Val AUC: 0.8948, Val F1: 0.7363
Epoch [45/100] - Train Loss: 0.0250, Val Loss: 0.0255, Val AUC: 0.8949, Val F1: 0.7534
Epoch [50/100] - Train Loss: 0.0250, Val Loss: 0.0252, Val AUC: 0.8946, Val F1: 0.7416

Early stopping at epoch 54

Training completed! Best AUC: 0.8957, Best F1: 0.7535

Training set evaluation results:
  Accuracy:    0.8272
  Precision:   0.8248
  Recall:      0.6740
  F1-Score:    0.7418
  Specificity: 0.9166
  ROC-AUC:     0.9002

============================================================
Validation Set Detailed Evaluation:
============================================================

Finding optimal classification threshold...
Best F1 Threshold: 0.4444, F1-Score: 0.7580

Using Threshold 0.5:
  Accuracy:    0.8194
  Precision:   0.8145
  Recall:      0.6600
  F1-Score:    0.7292
  Specificity: 0.9123
  ROC AUC:     0.8957
  PR AUC:      0.8516

Using Best Threshold 0.4444:
  Accuracy:    0.8122
  Precision:   0.7212
  Recall:      0.7987
  F1-Score:    0.7580
  Specificity: 0.8200

Confusion Matrix (Threshold=0.5):
[[9222  886]
 [2003 3889]]

  TN: 9222, FP: 886
  FN: 2003, TP: 3889

Classification Report (Threshold=0.5):
              precision    recall  f1-score   support

    Low Risk       0.82      0.91      0.86     10108
   High Risk       0.81      0.66      0.73      5892

    accuracy                           0.82     16000
   macro avg       0.82      0.79      0.80     16000
weighted avg       0.82      0.82      0.81     16000


ROC curve saved: transformer_model\outputs\roc_curve.png
PR curve saved: transformer_model\outputs\pr_curve.png
F1 threshold plot saved: transformer_model\outputs\f1_threshold.png
Confusion matrix saved: transformer_model\outputs\confusion_matrix.png

Training history saved: transformer_model\outputs\training_history.png

============================================================
Test Set Prediction:
============================================================

Starting test data preprocessing...
测试集特征维度: (20000, 40)
Prediction complete, number of samples: 20000
Predicted high risk ratio (threshold=0.5): 0.2987
Predicted high risk ratio (best threshold=0.444): 0.4106
Predicted probability range: [0.1297, 0.9384]

Prediction results saved: transformer_model\outputs\result.csv
File format: 20000 rows × 1 column
Model saved: transformer_model\outputs\transformer_model_final.pth

============================================================
Final Summary:
============================================================
Best ROC AUC:     0.8957
Best F1-Score:    0.7580 (Threshold=0.444)
Validation Precision: 0.7212
Validation Recall:    0.7987
============================================================

All tasks completed!
